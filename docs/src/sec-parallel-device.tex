%=========================================================================
% sec-parallel-device
%=========================================================================

\subsection{Parallelizing for the Accelerator Board}
\label{sec-parallel-device}

There are several ways we can leverage the Intel Xeon Phi accelerator
boards to further increase the speedups we see with the parallelized
implementation for the compute nodes. One approach is to execute the code
natively on the accelerator itself with no fine-grain
offloading. Although this approach makes it easier to tailor the code for
the accelerator and reduces overheads of copying data between the host
and the device memories, we are forced to run all the computation from
start to end on the accelerator. Another approach is to only offload
certain sections of the computation to the accelerator. With this
approach, there is a greater burden on the programmer to identify and
parallelize the most compute-intensive section of the code, but allows
him or her more flexibility in choosing which sections to accelerate.

As a starting point, we choose the second approach of offloading specific
sections of the computation to the accelerator and build off of the
parallel implementation for the compute nodes discussed in
Section~\ref{sec-parallel-node}. In this naive first-pass, we offload the
parallel section of the code and pass in the dimensions of the global
grid and the blocks, as well as a pointer to the global grid
itself. Inside the offloaded kernel, we still spawn off the specified
number of threads normally, except that instead of using pre-allocated
member \texttt{LocalState} objects, each thread creates its own \texttt{LocalState} object
on the stack. The \texttt{copy\_to\_local()} and
\texttt{copy\_from\_local()} functions were modified to copy the
flattened elements of the global grid passed in by the host to the local
grid vectors. Once the copy is complete, the other functions in the
offloaded kernel can take a pointer to the per-thread \texttt{LocalState} object
as an argument and access the vectors in this object similar to before.

Although this naive implementation requires us to offload computation
before every timestep, there is an unnecessarily high overhead of
transferring data between the host and device memories, but we believe
that using efficient batching will help us alleviate this
issue. Currently, the parallel implementation for the accelerator board
runs roughly an order of magnitude slower than the implementation for the
compute nodes with the same number of threads.
