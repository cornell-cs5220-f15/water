%=========================================================================
% sec-profile
%=========================================================================
\section{Profiling the Shallow Water Simulation}
\label{sec-profile}

Understanding which areas of the code most time is spent, and where opportunities for 
perfomance gains are available is key to achieving good speedups in general, and
in particular for our paralell implementation.

We will focus on run-time profiling of the code, as well as compile-time reports generated
by the Intel compiler to guide our efforts.

As we gain further understanding on the gode using these tools, we may expand to more detailed
and specific performance analysis tools such as \emph{TAU}, \emph{IACA}, and \emph{MAQAO}.

\subsection{Identifying Bottlenecks}
\label{sec-profile-bottlenecks}
As a first pass, we profiled the provided default code using the \emph{amplxe} tool, with 
truncated results given below.

\begin{lstlisting}
Function                                                  CPU Time
-------------------------------------------------------  -------- 
Central2D<Shallow2D, MinMod<float>>::limited_derivs      1.350s
Central2D<Shallow2D, MinMod<float>>::compute_step        0.652s
Central2D<Shallow2D, MinMod<float>>::compute_fg_speeds   0.236s 
\end{lstlisting}

Expectedly, the vast majority of the time is spent inside the functions for the limiter,
computing the step, and computing the wave speeds.  What is surprising is the amount of 
time spent inside the limiter.  Given a cursory glace at the code, one would assume that
the \texttt{compute\_step} function would be much more expensive than \texttt{limited\_derivs}, 
yet we are seeing just over 2 times as much time spent inside \texttt{limited\_derivs}.
Identifying the cause and improving this performance bottleneck should be a main priority.

One clue may be that the arithmetic intensity of \texttt{limited\_derivs} is much lower than
\texttt{compute\_step} and \texttt{compute\_fg\_speeds}.  We analyzed the binary with \emph{MAQAO},
and arithmetic intensity of the loops were computed.  The results can be seen in the table below.

\begin{center}
\begin{tabular}{ |c|c| } 
 \hline
 Function & Arithmetic Intensity (AI) \\ 
 \hline
 \texttt{limited\_derivs} & 0.11 \\ 
 \texttt{compute\_step} & 0.23-0.27 \\ 
 \texttt{compute\_fg\_speeds} & 0.21 \\ 
 \hline
\end{tabular}
\end{center}

\input{fig-roofline}

We can also look at more detailed profiling of individual functions.
As an example, below is an excerpt from a profiling report for the 
\texttt{compute\_step} function.

\begin{lstlisting}
// Predictor (flux values of f and g at half step)                                                             
for (int iy = 1; iy < ny_all-1; ++iy)                                                                          
    for (int ix = 1; ix < nx_all-1; ++ix) {                         0.002s
        vec uh = u(ix,iy);                                          0.013s 
        for (int m = 0; m < uh.size(); ++m) {                                                       
            uh[m] -= dtcdx2 * fx(ix,iy)[m];                                                         
            uh[m] -= dtcdy2 * gy(ix,iy)[m];                         0.024s
        }                                                                                           
        Physics::flux(f(ix,iy), g(ix,iy), uh);                                                      
    }                                                                                               
                                                                                                    
// Corrector (finish the step)                                                                      
for (int iy = nghost-io; iy < ny+nghost-io; ++iy)                                                   
    for (int ix = nghost-io; ix < nx+nghost-io; ++ix) {             0.010s 
        for (int m = 0; m < v(ix,iy).size(); ++m) {                                                 
            v(ix,iy)[m] =                                           0.115s
                0.2500 * ( u(ix,  iy)[m] + u(ix+1,iy  )[m] +        0.011s 
                           u(ix,iy+1)[m] + u(ix+1,iy+1)[m] ) -      0.034s 
                0.0625 * ( ux(ix+1,iy  )[m] - ux(ix,iy  )[m] +                                      
                           ux(ix+1,iy+1)[m] - ux(ix,iy+1)[m] +                                      
                           uy(ix,  iy+1)[m] - uy(ix,  iy)[m] +                                      
                           uy(ix+1,iy+1)[m] - uy(ix+1,iy)[m] ) -    0.009s
                dtcdx2 * ( f(ix+1,iy  )[m] - f(ix,iy  )[m] +                                        
                           f(ix+1,iy+1)[m] - f(ix,iy+1)[m] ) -      0.049s
                dtcdy2 * ( g(ix,  iy+1)[m] - g(ix,  iy)[m] +        0.004s
                           g(ix+1,iy+1)[m] - g(ix+1,iy)[m] );       0.044s 
        }                                                                                           
    }                                                                                               
                                                                                                    
// Copy from v storage back to main grid                                                            
for (int j = nghost; j < ny+nghost; ++j){                                                           
    for (int i = nghost; i < nx+nghost; ++i){                       0.004s
        u(i,j) = v(i-io,j-io);                                      0.010s
    }                                                                                                          
} 
\end{lstlisting}

Although not very enlightening, it verifies our intuition that the corrector
portion should be most heavily targetted for optimization, although a 
non-negligible amount of time is spent in other areas.

\subsubsection{Vectorization}
\label{sec-profile-vectorization}
Significant performance gains can be obtained by vectorizing our functions.
Vectorization is ``the unrolling of a loop combined with [...] SIMD instructions''
\footnote{\url{https://software.intel.com/sites/default/files/m/4/8/8/2/a/31848-CompilerAutovectorizationGuide.pdf}}.

We decided that rather than attempt to write SSE/AVX instructions by hand, we 
would rely on the autovectorization capabilities of the Intel compiler.
Guiding our efforts is the optimization/vectorization report generated by the intel
compiler.

Looking at the vectorization report generated by compiling the default code, we see
that the compiler did not vectorize any loop, with many message like this one, which
corresponds to the \texttt{compute\_fg\_speeds} function.

\begin{lstlisting}
   LOOP BEGIN at central2d.h(268,9)
      remark #15344: loop was not vectorized: vector dependence prevents 
                     vectorization
      remark #15346: vector dependence: assumed FLOW dependence between 
                     _M_elems line 74 and _M_elems line 76
      remark #15346: vector dependence: assumed ANTI dependence between 
                     _M_elems line 76 and _M_elems line 74
   LOOP END
\end{lstlisting}

Full definitions for the different types of vector dependence can be found in the 
Intel vectorization document linked above,
but the basic idea is that the compiler has to assume that arrays may refer to
overlapping memory locations.  We can invite the compiler to ignore this potential
dependency (if we as the programmers know it to be false), with the
\texttt{\#pragma ivdep} directive.

Doing so for the \texttt{compute\_fg\_speeds} results in the compiler
vectorizing the loop, and realizing a potential speedup of 3.84.

\begin{lstlisting}
   LOOP BEGIN at central2d.h(269,9)
      remark #15300: LOOP WAS VECTORIZED
      remark #15460: masked strided loads: 6 
      remark #15462: unmasked indexed (or gather) loads: 6 
      remark #15475: --- begin vector loop cost summary ---
      remark #15476: scalar loop cost: 308 
      remark #15477: vector loop cost: 76.870 
      remark #15478: estimated potential speedup: 3.840 
      remark #15479: lightweight vector operations: 109 
      remark #15481: heavy-overhead vector operations: 1 
      remark #15487: type converts: 8 
      remark #15488: --- end vector loop cost summary ---
   LOOP END
\end{lstlisting}

Full vectorization of the remaining code is not so simple.  The way the data is 
currently laid out, as an array of structs representing solution values, rather
than as a struct of arrays, makes it hard for the compiler for autovectorize.
Such a hierarchy is likely why Professor Bindel elected to re-write everything
in C -- a more in depth discussion of how to remain in C++ will be covered in
section~\ref{sec-tune-vectorizing}.

% \subsubsection{Run-time Bottlenecks}
% \label{sec-profile-bottlenecks-run-time}

% \subsection{Performance Modeling}
% \label{sec-profile-modeling}
